{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Later in this module, we're going to use a few very useful tools to architect and train neural networks. But before we dive into that, we need to understand much more basic concepts of neural networks. As you can see from the name, neural networks use *neurons*, or rather, a digital analog of a biological neuron. These neurons are linked or networked in a certain way to form a neural network. Essentially, a neural network is a computational model that works in a similar way as the human (or animal) brain.\n",
    "\n",
    "Neural networks range from the very simple single layer perceptron to complex ones with many hidden layers composed of many neurons. The complexity of the neural network depends on the dataset and the problem to solve for.\n",
    "\n",
    "We'll start with taking a closer look at the function of a neuron and how we use that to build a single layer perceptron.\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "A neuron can be thought of as a single unit that accepts input (data) and provides output. The value of the output depends on the *activation threshold* of the neuron. If the input is above the threshold, the neuron will \"fire\"; otherwise, the output is zero.\n",
    "\n",
    "The input to this perceptron is the data multiplied by a set of weights. The weights are updated for each interation or pass through the network. The weighted input is then passed to the *activation function*. This function maps the input to the output, which depends on the function. \n",
    "\n",
    "We start with some input data and a set of weights to apply to that data. We  The one part we haven't mentioned yet is how a neural network learns. On the first iteration, the weights are selected randomly. After the calculated results are compared to the expected results (using training data), the weights are adjusted. These new weights are used to calculate the weight sum, new results are calculated, and so on, for the number of iterations specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's implement a simple single layer perceptron to better explain what we learned in the overview. It will be easier to understand how the different components of a neural network fit together with some example code. \n",
    "\n",
    "In the following example, we'll describe and implement the following parts of a neural network:\n",
    "\n",
    "* activation function\n",
    "* input data\n",
    "* weights\n",
    "* learning rate\n",
    "\n",
    "### Activation function\n",
    "\n",
    "The activation function maps the input to the output. This example uses a *step function* where the output is 0 if the sum of the weighted input is less than 0, and 1 otherwise. A visualization of the function will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the activation function\n",
    "unit_step = lambda x: 0 if x < 0 else 1\n",
    "\n",
    "# Vectorize the function (use on an array)\n",
    "unit_step_v = np.vectorize(unit_step)\n",
    "\n",
    "# Create arrays to plot\n",
    "x = np.arange(-5, 5, 0.1)\n",
    "y = unit_step_v(x)\n",
    "\n",
    "# Plot\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('input'); plt.ylabel('step function output');\n",
    "\n",
    "plt.clf() # comment/delete to show plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mod1_obj1_stepfunc.png](https://raw.githubusercontent.com/LambdaSchool/data-science-canvas-images/main/unit_4/sprint_2/mod1_obj1_stepfunc.png)\n",
    "\n",
    "Next, we'll define some data to test, which in this case will be the logical OR operator: for input that includes a 1, the output is 1, otherwise the output is 0. We'll consider an input array of two values so the possible choices are: `[0,0], [0,1], [1,0], [1,1]`. There is also a bias term, which is currently set to 1 for all inputs (the bias can be used to adjust the threshold; we're going to focus on the weights only first). The other part of the training data is the expected output, which is 0 for `[0,0]` and 1 for the rest of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ('OR' gate)\n",
    "# tuple format: ([x1, x2, bias], expected)\n",
    "training_data = [\n",
    "    (np.array([0,0,1]), 0),\n",
    "    (np.array([0,1,1]), 1),\n",
    "    (np.array([1,0,1]), 1),\n",
    "    (np.array([1,1,1]), 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll code the perceptron. First, we'll initialize the weights using random numbers between 0 and 1. Then we'll set the learning rate as 0.2 (we'll learn more about learning rate in the next module). We'll start with a low number of iterations so we can easily look through our output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]: -0.09972348640877343 -> 0\n",
      "[0 1]: 0.633699260921845 -> 1\n",
      "[1 0]: 0.5182908336831622 -> 1\n",
      "[1 1]: 1.2517135810137805 -> 1\n"
     ]
    }
   ],
   "source": [
    "# Perceptron code follows the example here, with\n",
    "# some modifications: \n",
    "# https://blog.dbrgn.ch/2013/3/26/perceptrons-in-python/\n",
    "\n",
    "# Imports\n",
    "#for random input selection\n",
    "from random import choice \n",
    "\n",
    "# Weights (begin with random weights)\n",
    "w = np.random.rand(3)\n",
    "\n",
    "# Errors (store for plotting)\n",
    "errors = []\n",
    "\n",
    "# Learning rate (the size of \"jumps\" when updating the weights)\n",
    "learn_rate = 0.2\n",
    "\n",
    "# Number of iterations/weight updates\n",
    "n = 50\n",
    "\n",
    "# \"Learning\" loop\n",
    "for i in range(n):\n",
    "    \n",
    "    # Select a random item from the training data\n",
    "    x, expected = choice(training_data)\n",
    "    \n",
    "    # Neuron calculation (dot product of weights and input)\n",
    "    result = np.dot(w, x)\n",
    "    \n",
    "    # Compare to the expected result\n",
    "    error = expected - unit_step(result)\n",
    "    errors.append(error)\n",
    "    \n",
    "    # Update the weights\n",
    "    w += learn_rate * error * x\n",
    "\n",
    "# Test the perceptron with the \"learned\" weights\n",
    "for x, _ in training_data:\n",
    "    result = np.dot(x, w)\n",
    "    print(\"{}: {} -> {}\".format(x[:2], result, unit_step(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made a neural network! It learned to predict a 0 when the input was `[0,0]` and 1 otherwise. We completed 50 iterations, which seems like plenty to learn this simple model. We'll plot the error as a function of iteration and see when it stays at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot error as a function of iteration\n",
    "iteration = np.arange(0, n, 1)\n",
    "plt.plot(iteration, errors)\n",
    "plt.xlabel('iteration'); plt.ylabel('error');\n",
    "\n",
    "plt.clf() # comment/delete to show plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mod1_obj1_error.png](https://raw.githubusercontent.com/LambdaSchool/data-science-canvas-images/main/unit_4/sprint_2/mod1_obj1_error.png)\n",
    "\n",
    "The error stays at zero after about 20 iterations (though this will change because of the random weights chosen at the start), which seems reasonable for a small dataset and a relatively simple model to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "There are a few parameters that can be adjusted in the above example.But first, you might want to wrap the perceptron in a function and then try out different parameters. First, try using a different activation function; the [sigmoid](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid) is one that is commonly used. Another parameter that could be adjusted is the learning rate; what changes if you set this to a larger values, such as 0.75? Finally, try a different training dataset, such as the logical `AND` operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [3Blue1Brown: But what is a Neural Network? | Deep learning, chapter 1](https://www.youtube.com/watch?v=aircAruvnKk&t=6s)\n",
    "* [First neural network for beginners explained (with code)](https://medium.com/@thomascountz/perceptrons-in-neural-networks-dc41f3e4c1b9)\n",
    "* [Perceptrons in Neural Networks](https://medium.com/@thomascountz/perceptrons-in-neural-networks-dc41f3e4c1b9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
