{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In any natural language, there are many words that share the same root word. For example, think about the root *auto* which means *self*. When combined with other words, we have automatic, automobile, autocrat, and many others. Or the word *active*. We can't reduce these words down any further and still have meaning. But these words are also *stems*. We can add endings (auto*matically*, auto*mobiles*, activa*ted*) or a prefix (*in*active) or both (*de*activa*ted*).\n",
    "\n",
    "In language, this process is called inflection where a word is modified for different uses including tensem case number, and gender. But inflected words still retain their core meaning.\n",
    "\n",
    "In English (and other languages) inflection is applied to a verb and is called verb conjugation. We modify verbs for person, tense, and number. An example verb conjugation for tense would be: work, worked, am working.\n",
    "\n",
    "Nouns are infected for number (plural) by changing the sufix: cat-cats, wolf-wolves, puppy-puppies. Just to make things confusing, sometimes more than the suffix is changes, like for goose-geese or mouse-mice.\n",
    "\n",
    "In natural language processing, we use stemming or lemmatization to trim our words down to the root word or stem. As we can see from the inflection examples, language is complicated and there are no simple rules to do this trimming. Let's look in more detail at the process to get a better idea of how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "As we've already see from the inflection examples above, the process of stemming can involve something simple like removing and \"s\" or \"es\" from the end of a noun or a \"ed\" or \"ing\" from the end of a verb.\n",
    "\n",
    "These rules aren't comprehensive but are a good starting place. Fortunately, there is a lot of research on stemming algorithms. For more information on various stemming algorithms, check out the resources listed below.\n",
    "\n",
    "Let's look in more detail at the Porter stemmer. It works by using an explicit list of suffixes and a list of criteria under which that suffix can be removed. The stemmer works in phases where each phase follows a particular rule, depending on the end of the word. For example, the first phase uses the following \"rule group\".\n",
    "\n",
    "| Rule    |   |       | Example  |   |          |\n",
    "| --------|---|-------|----------|---|----------|\n",
    "| SSES |&rarr;| SS   | caresses |&rarr;| caress |\n",
    "| IES |&rarr;| I     | ponies |&rarr;| poni     |\n",
    "| SS |&rarr;| SS     | caress |&rarr;| caress   |\n",
    "| S |&rarr;|         | cats |&rarr;| cat        |\n",
    "\n",
    "We can see that the result stem on the right doesn't have to be an actual word (\"poni\") because we can still understand the meaning. Here's an example of what some text looks like after going through the Porter stemmer.\n",
    "\n",
    "We can see that the result stem on the right doesn't have to be an actual word (\"poni\") because we can still understand the meaning. Here's an example of what some text looks like after going through the Porter stemmer.\n",
    "\n",
    "```plaintext\n",
    "> *Sample text:* Such and analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation\n",
    "> *Porter stemmer:* such an analysi can reveal featur that ar not easili visibl from the variat in the individu gene and can lead to a pictur of express that is more biolog transpar and access to interpret\n",
    "```\n",
    "\n",
    "A Porter stemmer can be implemented in Python with the Natural Languages Toolkit (NLTK) but we are going to be working with the spaCy library for this part of the course. However, spaCy doesn't do stemming out of the box, but instead uses a different technique called *lemmatization* which we'll discuss next.\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "As you read through the example sentence after stemming, it will seem like the words are literally \"chopped off\". The job of a stemmer is to remove the endings, which is essentially what the Porter stemmer example above shows. This type of stemming works well when the result doesn't need to be human-readable. Stemming is useful in search and information retrieval applications; also, it's fast.\n",
    "\n",
    "Lemmatization on the other hand is more methodical. The goal is to transform a word into its base form called a *lemma*. Plural nouns with uncommon spellings get transformed to the singular tense. Verbs are all transformed to the transitive: an action word with something or someone receiving the action such as *paint* (transitive verb) the *canvas* (object).\n",
    "\n",
    "However, this type of processing has a computational cost. In this case, spaCy does a pretty good job of lemmatizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's use some of our example text from the previous objective to apply lemmatization to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  -->  the\n",
      "rabbit  -->  rabbit\n",
      "-  -->  -\n",
      "hole  -->  hole\n",
      "went  -->  go\n",
      "straight  -->  straight\n",
      "on  -->  on\n",
      "like  -->  like\n",
      "a  -->  a\n",
      "tunnel  -->  tunnel\n",
      "for  -->  for\n",
      "some  -->  some\n",
      "way  -->  way\n",
      ",  -->  ,\n",
      "and  -->  and\n",
      "then  -->  then\n",
      "dipped  -->  dip\n",
      "suddenly  -->  suddenly\n",
      "down  -->  down\n",
      ",  -->  ,\n",
      "so  -->  so\n",
      "suddenly  -->  suddenly\n",
      "that  -->  that\n",
      "Alice  -->  Alice\n",
      "had  -->  have\n",
      "not  -->  not\n",
      "a  -->  a\n",
      "moment  -->  moment\n",
      "to  -->  to\n",
      "think  -->  think\n",
      "about  -->  about\n",
      "stopping  -->  stop\n",
      "herself  -->  -PRON-\n",
      "before  -->  before\n",
      "she  -->  -PRON-\n",
      "found  -->  find\n",
      "herself  -->  -PRON-\n",
      "falling  -->  fall\n",
      "down  -->  down\n",
      "a  -->  a\n",
      "very  -->  very\n",
      "deep  -->  deep\n",
      "well  -->  well\n",
      ".  -->  .\n"
     ]
    }
   ],
   "source": [
    "# Import the library\n",
    "import spacy\n",
    "\n",
    "# Create an example sentence\n",
    "sent = \"The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well.\"\n",
    "\n",
    "# Load the language library\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Lemma Attributes\n",
    "for token in doc:\n",
    "    print(token.text, \" --> \", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we go! We have tokenized *and* lemmatized our first sentence in spaCy. The lemmas are much easier to read than the stemmed text. In this particular there isn't a lots of \n",
    "\n",
    "In order to make this process more efficient, let's put these various text normalizing functions in another function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and lemmatizing in one function\n",
    "def get_lemmas(text):\n",
    "\n",
    "    # Initialize a list\n",
    "    lemmas = []\n",
    "    \n",
    "    # Convert the input text into a spaCy doc\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Remove stop words, punctuation, and personal pronouns (PRON)\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    # Return the lemmatized tokens\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text (https://en.wikipedia.org/wiki/Geology)\n",
    "geology = [\"Geology describes the structure of the Earth on and beneath its surface, and the processes that have shaped that structure.\",\n",
    "        \"It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks.\", \n",
    "        \"By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth.\",\n",
    "        \"Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['geology', 'describe', 'structure', 'Earth', 'beneath', 'surface', 'process', 'shape', 'structure'], ['provide', 'tool', 'determine', 'relative', 'absolute', 'age', 'rock', 'find', 'give', 'location', 'describe', 'history', 'rock'], ['combine', 'tool', 'geologist', 'able', 'chronicle', 'geological', 'history', 'Earth', 'demonstrate', 'age', 'Earth'], ['geology', 'provide', 'primary', 'evidence', 'plate', 'tectonic', 'evolutionary', 'history', 'life', 'Earth', 'past', 'climate']]\n"
     ]
    }
   ],
   "source": [
    "# Find the lemmas for each sentence in the above text\n",
    "geology_lemma = [get_lemmas(sentence) for sentence in geology]\n",
    "print(geology_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to the original sentences, we see lots of words that have been changed, either by becoming singular or changing the tense: processes to process, shaped to shape, combining to combine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Using the `get_lemmas()` function above, complete the following text normalization steps on any text you choose. The text can be something more technical from Wikipedia or your favorite book.\n",
    "\n",
    "* tokenization\n",
    "* removing stop words\n",
    "* remove pronouns (part of speech)\n",
    "* lemmatization\n",
    "\n",
    "Compare your original text with the lemmatized version and see if you can notice any patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [Stemming algorithms](https://pdfs.semanticscholar.org/1c0c/0fa35d4ff8a2f925eb955e48d655494bd167.pdf)\n",
    "* [Porter stemmer](http://people.scs.carleton.ca/~armyunis/projects/KAPI/porter.pdf)\n",
    "* [spaCy: Text Processing (Lemmatization)](https://spacy.io/api/annotation#text-processing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
