{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the previous module, we did some basic document classification using latent semantic analysis (LSA). With that method, we found common \"concepts\" in documents and compared them to other documents to find similarities. In this next module, we're going to go deeper into learning more about *topics* in our documents.\n",
    "\n",
    "### Topics\n",
    "\n",
    "First, let's define what a topic is. A piece of text is usually written about a specific topic, like astronomy or cats or cooking. We would expect to find similar words in any documents that are about the same topic. Articles and books about astronomy would include terms like \"planet\", \"orbit\", and \"telescope\". A text about cooking would have different words; for example, \"mixing\", \"oven\", and \"diced\" are common terms used in cooking. But, these documents on two different topics would also have words in common, such as \"temperature\", \"scale\", and \"measure\".\n",
    "\n",
    "So how do we determine which \"topics\" are represented in a document? Math! We can use various mathematical and statistical techniques to find out what these topics are and their representation in a text. This module will focus on a technique called latent Dirichlet allocation (LDA).\n",
    "\n",
    "### Latent Dirichlet allocation\n",
    "\n",
    "The basis of the LDA model is that it assumes that a document is a mixture of topics and that all the words in the document (after removing stop words and stemming/lemmatization) belong to a topic. A simpler way might be to say each document is a mixture of topics and each topic is a mixture of words.\n",
    "\n",
    "More generally, LDA is an unsupervised learning (clustering) technique, where the clusters are the topics. You can also think about representing a document in \"topic space\" in the same way that we use word embeddings to represent a word in a vector space.\n",
    "\n",
    "One of the parameters to specify when fitting an LDA model is the number of topic. This isn't something that can be measured or determined before actually assigning words to topics. But, the number of topics can be optimized by determining how well different numbers of topics by measuring the performance during a classification or regression task.\n",
    "\n",
    "In the next objective, we'll dive into creating topic models with some Python packages. But for now, we'll just look at the simple topics that are found for a small collection of documents. By this, we mean five sentences! Let's find some topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "\n",
    "Let's pretend we want to find the topics in this corpus:\n",
    "\n",
    "1. \"Many people have believed that the summer season is hot because the Earth is closer to the Sun.\"\n",
    "2. \"Would the state of liquid water change to a solid or gas on the surface of the Moon?\"\n",
    "3. \"After discussing their problems, the underlying issue began to surface.\"\n",
    "4. \"The recipe did state specific cooking directions but doesn't indicate how to season the food.\"\n",
    "5. \"The state needs to issue the document.\"\n",
    "\n",
    "There are some words that have multiple meanings such as season (time of year or food flavoring), state (form of matter or geographic location), and issue (to give out or problem/difficulty).\n",
    "\n",
    "What topics can we identify in these documents? We have some statements that are related to science topics (Earth, Sun, state of liquid) and other sentences that refer to more general topics like cooking or paperwork.\n",
    "\n",
    "Let's see what topics are produced if we fit an LDA model to this text. We have to choose the number of topics so for a small sample like this, we'll choose a small number of topics like three. FOr each topic, we'll look at the top three words. There are obviously more than nine words (`3*3`) in our corpus, but for now, three should be enough to get an idea of what the topic is.\n",
    "\n",
    "Don't worry about the details of fitting the model right now; we'll get to this in the next objective.\n",
    "\n",
    "### Topics\n",
    "\n",
    "|topic|word1|word2|word3|\n",
    "|-----|---------|----------|---------|\n",
    "|0    |state    |water     |liquid   |\n",
    "|1    |season   |people    |believed |\n",
    "|2    |issue    |surface   |underlying|\n",
    "\n",
    "\n",
    "We have our three topics and the words associated with those topics. THe first topic seems like we could call it \"science\" since state seems to be associated with the scientific definition. The second topic is more difficult to try to classify, but \"season\" seems to be associated with things people do rather than the weather. And the last topic could be called \"psychology\" as it seems those are terms a counselor might use.\n",
    "\n",
    "Even with this small example, we're starting to get a feel for what a topic is, especially when we can easily see and read the whole corpus. Usually we'll be dealing with large amounts of text and the topics might make as much sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Using the topic modeling application available at this [website](https://lettier.com/projects/lda-topic-modeling/), try adding some of you own documents. Suggestions for what to use here include:\n",
    "\n",
    "* A paragraph of text from an open source text book or non-fiction article (NASA, science news letters, etc.)\n",
    "* Excerpts from novels by different authors from public domain texts on [Project Gutenberg](https://www.gutenberg.org/)\n",
    "* A sample of your own writing on a topic compared to a sample from a different source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [LDA Topic Modeling](https://lettier.com/projects/lda-topic-modeling/)\n",
    "* [Your Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
