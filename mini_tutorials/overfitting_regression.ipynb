{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Earlier in this module, we introduced how to separate our data into training and testing sets. The main reason to do this is so we can verify the results of our model on new data or data the model hasn't yet seen. The more formal term is *overfitting* or producing a model that performs well on our training data but doesn't generalize well to our test data (or other new data). \n",
    "\n",
    "The opposite problem also occurs, where we can *underfit* a model meaning it doesn't do well even on the training data. In this case, you would probably choose a different model to fit so underfitting is not as much of a problem as overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "\n",
    "### Example: overfitting vs. underfitting\n",
    "\n",
    "To better understand model fitting, let's use an example where we have fit a polynomial (curve) to a small data set. There are three polynomials fit to the data: a first degree (which is just a line), a sixth degree, and a ninth degree. The degree of a polynomial indicates how many nonzero coefficients it has; the higher the degree, the more parameters can be fit to the data.\n",
    "\n",
    "![over-under](https://raw.githubusercontent.com/LambdaSchool/data-science-canvas-images/main/unit_2/sprint_1/mod2_obj4_overunder.png)\n",
    "\n",
    "The line already fits the data reasonably well, but is still probably underfit. The sixth degree polynomial is an example of overfitting, where the model will probably not fit new data very well. The ninth degree polynomial nicely goes through every data point but needs to make several dips to fit; this model would provide a very poor result when used on new data.\n",
    "\n",
    "### Bias and variance\n",
    "\n",
    "The concepts of underfitting and overfitting are also related to *bias* and *variance*. A high bias model makes a lot of mistakes in fitting the model; the first degree polynomial model is example. But, if we fit this 0-degree model to a different training set, it will likely have similar results. This type of model has a high bias and low variance.\n",
    "\n",
    "On the other end, the 9-degree model fits the data almost perfectly. But, if we trained the model on a different training set, the results would be very different; the curve would not look the same. This type of model has low bias but a high variance.\n",
    "\n",
    "* High bias: Doesn't pay a lot of attention to the data and over simplifies the model\n",
    "* Low bias: Pays too much attention to the data and is usually a complicated model\n",
    "* High variance: Fits the training data set very well but doesn't generalize to new data\n",
    "* Low variance: Returns similar models for different sets of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [Ask a Data Scientist: The Bias vs. Variance Tradeoff](https://insidebigdata.com/2014/10/22/ask-data-scientist-bias-vs-variance-tradeoff/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
